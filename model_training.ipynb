{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123d1aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa) (4.11.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: noisereduce in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (1.13.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (4.66.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (1.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from tqdm->noisereduce) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: noisereduce in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: sounddevice in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (1.13.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (4.66.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (1.4.2)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from sounddevice) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from tqdm->noisereduce) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting audiomentations\n",
      "  Downloading audiomentations-0.40.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from audiomentations) (1.26.4)\n",
      "Collecting numpy-minmax<1,>=0.3.0 (from audiomentations)\n",
      "  Downloading numpy_minmax-0.4.0-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting numpy-rms<1,>=0.4.2 (from audiomentations)\n",
      "  Downloading numpy_rms-0.5.0-cp312-cp312-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting librosa!=0.10.0,<0.11.0,>=0.8.0 (from audiomentations)\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting python-stretch<1,>=0.3.1 (from audiomentations)\n",
      "  Downloading python_stretch-0.3.1-cp312-abi3-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: scipy<2,>=1.4 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from audiomentations) (1.13.1)\n",
      "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from audiomentations) (0.5.0.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.5.1)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.8.2)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.11.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.0.3)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (1.17.1)\n",
      "INFO: pip is looking at multiple versions of numpy-minmax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy-minmax<1,>=0.3.0 (from audiomentations)\n",
      "  Downloading numpy_minmax-0.3.1-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "INFO: pip is looking at multiple versions of numpy-rms to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy-rms<1,>=0.4.2 (from audiomentations)\n",
      "  Downloading numpy_rms-0.4.2-cp312-cp312-win_amd64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.21)\n",
      "Requirement already satisfied: packaging in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from lazy-loader>=0.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (24.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2024.12.14)\n",
      "Downloading audiomentations-0.40.0-py3-none-any.whl (83 kB)\n",
      "Downloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
      "Downloading numpy_minmax-0.3.1-cp312-cp312-win_amd64.whl (14 kB)\n",
      "Downloading numpy_rms-0.4.2-cp312-cp312-win_amd64.whl (13 kB)\n",
      "Downloading python_stretch-0.3.1-cp312-abi3-win_amd64.whl (97 kB)\n",
      "Installing collected packages: python-stretch, numpy-rms, numpy-minmax, librosa, audiomentations\n",
      "  Attempting uninstall: librosa\n",
      "    Found existing installation: librosa 0.11.0\n",
      "    Uninstalling librosa-0.11.0:\n",
      "      Successfully uninstalled librosa-0.11.0\n",
      "Successfully installed audiomentations-0.40.0 librosa-0.10.2.post1 numpy-minmax-0.3.1 numpy-rms-0.4.2 python-stretch-0.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: noisereduce in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (1.13.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (4.66.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from noisereduce) (1.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from tqdm->noisereduce) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install librosa\n",
    "%pip install noisereduce\n",
    "%pip install noisereduce sounddevice\n",
    "%pip install audiomentations\n",
    "%pip install noisereduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89cc771",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'audiomentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiomentations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Compose, AddGaussianNoise, PitchShift, TimeStretch\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtempfile\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'audiomentations'"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch\n",
    "import random\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7369f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of unzipping the zip file of dataset.\n",
    "\n",
    "def extract_zip_dataset(zip_path, extract_path):\n",
    "\n",
    "    # Create the extraction directory if it does not exist\n",
    "    if not os.path.exists(extract_path):\n",
    "        os.makedirs(extract_path)\n",
    "\n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "    print(f\"Dataset extracted to {extract_path}\")\n",
    "    return extract_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b813f471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted to /content/unzipped\n"
     ]
    }
   ],
   "source": [
    "# Unzipping the Zipped dataset of Echo-Heist.\n",
    "zip_path = \"Audio_Sentiment.zip\"\n",
    "extract_path = \"/content/unzipped\"\n",
    "base_folder = extract_zip_dataset(zip_path, extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now we are removing the Background Noise and saving audio file as a WAV file format\n",
    "\n",
    "def reduce_noise_and_convert(file_path, output_path):\n",
    "        # Loading the  audio file\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Denoising by the \"noisereduce library\"\n",
    "        y_denoised = nr.reduce_noise(y=y, sr=sr)\n",
    "\n",
    "        # Normalizing the Denoised audio file\n",
    "        y_denoised = librosa.util.normalize(y_denoised)\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # Save as .wav\n",
    "        sf.write(output_path, y_denoised, sr)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "# Finding each file & Denoising them.\n",
    "def find_and_process_files(input_dir, output_dir, extensions=None):\n",
    "\n",
    "    if extensions is None:\n",
    "        extensions = ['.wav','.amr', '.mpeg']\n",
    "\n",
    "    audio_files = []\n",
    "\n",
    "    # Gather all valid audio files\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in extensions):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "\n",
    "    print(f\"We have processed {len(audio_files)} audio files.\")\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for file_path in tqdm(audio_files, desc=\"Processing\"):\n",
    "        # Get relative path and convert extension to .wav\n",
    "        relative_path = os.path.relpath(file_path, input_dir)\n",
    "        relative_path = os.path.splitext(relative_path)[0] + \".wav\"\n",
    "        output_path = os.path.join(output_dir, relative_path)\n",
    "\n",
    "        if reduce_noise_and_convert(file_path, output_path):\n",
    "            count += 1\n",
    "\n",
    "    print(f\"\\n  We have successfully processed {count}/{len(audio_files)} files.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"/content/unzipped\"\n",
    "    output_folder = \"denoised_final_folder\"\n",
    "    find_and_process_files(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Dictionary for emotions' label\n",
    "emotion_labels = {\n",
    "    'sad': 0,\n",
    "    'surprised': 1,\n",
    "    'joyfully': 2,\n",
    "    'euphoric': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cacbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Extracting the Features from an Audio File\n",
    "\n",
    "def extract_features(y, sr, max_pad_len=150):\n",
    "    try:\n",
    "        y = librosa.util.normalize(y)\n",
    "        y_trimmed, _ = librosa.effects.trim(y, top_db=20)\n",
    "\n",
    "        mfccs = librosa.feature.mfcc(y=y_trimmed, sr=sr, n_mfcc=20)\n",
    "        mfccs_normalized = (mfccs - np.mean(mfccs, axis=1, keepdims=True)) / np.std(mfccs, axis=1, keepdims=True)\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y_trimmed, sr=sr, n_mels=40, fmax=8000)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        mel_spec_normalized = (mel_spec_db - np.mean(mel_spec_db)) / np.std(mel_spec_db)\n",
    "\n",
    "        features = np.vstack([mfccs_normalized, mel_spec_normalized])\n",
    "\n",
    "        if features.shape[1] > max_pad_len:\n",
    "            start = (features.shape[1] - max_pad_len) // 2\n",
    "            features = features[:, start:start + max_pad_len]\n",
    "        else:\n",
    "            pad_width = max_pad_len - features.shape[1]\n",
    "            features = np.pad(features, pad_width=((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545dbd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder of loading The Dataset and if sampling-rate is not matching then resampling it\n",
    "\n",
    "def load_audio(file_path, target_sr=22050):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        if len(y) < sr * 0.5:\n",
    "            print(f\"Warning: Audio file too short: {file_path}\")\n",
    "            return None, None\n",
    "        if sr != target_sr:\n",
    "            y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "        return y, target_sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {file_path}: {e}\")\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e70e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset from base_folder & Extracting the features\n",
    "\n",
    "def load_dataset(base_folder, target_sr=22050):\n",
    "    data, labels, file_paths = [], [], []\n",
    "    emotion_files = {emotion: 0 for emotion in emotion_labels.keys()}\n",
    "    total_files = 0\n",
    "    valid_files = 0\n",
    "\n",
    "    print(f\"Loading dataset from {base_folder}.\")\n",
    "\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.wav','.amr','.mpeg')):\n",
    "                total_files += 1\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_emotion = None\n",
    "                for emotion in emotion_labels.keys():\n",
    "                    if emotion in file.lower() or emotion in os.path.basename(root).lower():\n",
    "                        file_emotion = emotion\n",
    "                        break\n",
    "                if file_emotion:\n",
    "                    y, sr = load_audio(file_path, target_sr)\n",
    "                    if y is not None:\n",
    "                        features = extract_features(y, sr)\n",
    "                        if features is not None:\n",
    "                            data.append(features)\n",
    "                            labels.append(emotion_labels[file_emotion])\n",
    "                            file_paths.append(file_path)\n",
    "                            emotion_files[file_emotion] += 1\n",
    "                            valid_files += 1\n",
    "\n",
    "    print(f\"Processed {total_files} files, {valid_files} valid files used.\")\n",
    "    for emotion, count in emotion_files.items():\n",
    "        print(f\"  - {emotion}: {count} files\")\n",
    "\n",
    "    return np.array(data), np.array(labels), file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Augmentation of data bcz it's very less\n",
    "def create_augmentation_pipeline():\n",
    "    return Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.01, p=0.5),\n",
    "        TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),\n",
    "        PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n",
    "    ])\n",
    "\n",
    "def generate_augmented_data(file_paths, labels, target_count=100):\n",
    "    print(\"Start generating the augmented data...\")\n",
    "    emotion_files = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in emotion_files:\n",
    "            emotion_files[label] = []\n",
    "        emotion_files[label].append(file_paths[i])\n",
    "\n",
    "    class_counts = np.bincount(labels)\n",
    "    print(\"Original class distribution:\", class_counts)\n",
    "\n",
    "    augmentation = create_augmentation_pipeline()\n",
    "    augmented_features, augmented_labels = [], []\n",
    "\n",
    "    for label, files in emotion_files.items():\n",
    "        current_count = class_counts[label]\n",
    "        if current_count < target_count:\n",
    "            num_needed = target_count - current_countc\n",
    "            print(f\"Class {label}: Adding {num_needed} augmented samples\")\n",
    "\n",
    "            for _ in tqdm(range(num_needed)):\n",
    "                file_path = random.choice(files)\n",
    "                y, sr = load_audio(file_path)\n",
    "                if y is None:\n",
    "                    continue\n",
    "                augmented_audio = augmentation(samples=y, sample_rate=sr)\n",
    "                features = extract_features(augmented_audio, sr)\n",
    "                if features is not None:\n",
    "                    augmented_features.append(features)\n",
    "                    augmented_labels.append(label)\n",
    "\n",
    "    return np.array(augmented_features), np.array(augmented_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now building the Deep-Learning Model.\n",
    "\n",
    "def build_optimized_cnn(input_shape, num_classes):\n",
    "\n",
    "    regularizer = regularizers.l2(0.001)\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "\n",
    "        layers.Conv2D(32, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizer),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizer),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizer),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    initial_learning_rate = 0.001\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing the Data for Training the Model.\n",
    "\n",
    "def train_with_mixup(model, X_train, y_train, X_val, y_val, batch_size=32, epochs=100):\n",
    "    def mixup(x, y, alpha=0.2):\n",
    "        lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        indices = tf.random.shuffle(tf.range(batch_size))\n",
    "        mixed_x = lam * x + (1 - lam) * tf.gather(x, indices)\n",
    "        mixed_y = lam * y + (1 - lam) * tf.gather(y, indices)\n",
    "        return mixed_x, mixed_y\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x, y):\n",
    "        mixed_x, mixed_y = mixup(x, y)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(mixed_x, training=True)\n",
    "            loss = loss_fn(mixed_y, y_pred)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        train_acc_metric.update_state(mixed_y, y_pred)\n",
    "        return loss\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1024).batch(batch_size)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "\n",
    "    history = {\"loss\": [], \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        train_acc_metric.reset_state()\n",
    "        losses = []\n",
    "        for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "            loss = train_step(x_batch, y_batch)\n",
    "            losses.append(float(loss))\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        train_acc = train_acc_metric.result()\n",
    "\n",
    "        val_losses, val_accs = [], []\n",
    "        for x_val_batch, y_val_batch in val_dataset:\n",
    "            val_pred = model(x_val_batch, training=False)\n",
    "            val_loss = loss_fn(y_val_batch, val_pred)\n",
    "            val_losses.append(float(val_loss))\n",
    "            val_acc = tf.keras.metrics.categorical_accuracy(y_val_batch, val_pred)\n",
    "            val_accs.append(float(tf.reduce_mean(val_acc)))\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        avg_val_acc = sum(val_accs) / len(val_accs)\n",
    "\n",
    "        history[\"loss\"].append(avg_loss)\n",
    "        history[\"accuracy\"].append(float(train_acc))\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "        history[\"val_accuracy\"].append(avg_val_acc)\n",
    "\n",
    "        print(f\"Loss: {avg_loss:.4f}, Accuracy: {float(train_acc):.4f}, \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_acc:.4f}\")\n",
    "\n",
    "        if avg_val_acc > best_val_acc:\n",
    "            best_val_acc = avg_val_acc\n",
    "            patience_counter = 0\n",
    "            model.save('best_emotion_model.h5')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 20:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    model = tf.keras.models.load_model('best_emotion_model.h5')\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61300f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Starting speech emotion recognition training...\")\n",
    "    base_folder = \"/content/denoised_final_folder\"\n",
    "    X, y, file_paths = load_dataset(base_folder)\n",
    "\n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No data loaded. Exiting.\")\n",
    "        return\n",
    "\n",
    "    X_aug, y_aug = generate_augmented_data(file_paths, y, target_count=150)\n",
    "\n",
    "    if len(X_aug) > 0:\n",
    "        X_combined = np.vstack([X, X_aug])\n",
    "        y_combined = np.concatenate([y, y_aug])\n",
    "    else:\n",
    "        X_combined = X\n",
    "        y_combined = y\n",
    "\n",
    "    print(f\"Final dataset: {X_combined.shape[0]} samples\")\n",
    "    print(f\"Class distribution: {np.bincount(y_combined)}\")\n",
    "\n",
    "    X_reshaped = X_combined.reshape(X_combined.shape[0], X_combined.shape[1], X_combined.shape[2], 1)\n",
    "    y_onehot = tf.keras.utils.to_categorical(y_combined, num_classes=len(emotion_labels))\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_reshaped, y_onehot, test_size=0.2, random_state=42, stratify=y_combined)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "    model = build_optimized_cnn(input_shape, len(emotion_labels))\n",
    "    model.summary()\n",
    "\n",
    "    model, history = train_with_mixup(model, X_train, y_train, X_val, y_val, batch_size=16, epochs=150)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    class_names = [k for k, v in sorted(emotion_labels.items(), key=lambda item: item[1])]\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n",
    "\n",
    "\n",
    "# CHecking the confusion metrics\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# graph of\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['accuracy'], label='Train')\n",
    "    plt.plot(history['val_accuracy'], label='Validation')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    model.save('finishing__model.h5')\n",
    "    print(\"Model saved as 'finishing_touches_model.h5'\")\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2625342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### prediction code \n",
    "\n",
    "def predict_emotion(audio_file_path, model_path='/content/best_emotion_model.h5'):\n",
    "    # Define emotion labels\n",
    "    emotion_labels = {\n",
    "        'sad': 0,\n",
    "        'surprised': 1,\n",
    "        'joyfully': 2,\n",
    "        'euphoric': 3\n",
    "    }\n",
    "    # Reverse the dictionary for prediction output\n",
    "    idx_to_emotion = {v: k for k, v in emotion_labels.items()}\n",
    "\n",
    "    # Load the model\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # Helper functions from your training script\n",
    "    def load_audio(file_path, target_sr=22050):\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "            if len(y) < sr * 0.5:\n",
    "                print(f\"Warning: Audio file too short: {file_path}\")\n",
    "                return None, None\n",
    "            if sr != target_sr:\n",
    "                y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "            return y, target_sr\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {file_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def extract_features(y, sr, max_pad_len=150):\n",
    "        try:\n",
    "            y = librosa.util.normalize(y)\n",
    "            y_trimmed, _ = librosa.effects.trim(y, top_db=20)\n",
    "\n",
    "            mfccs = librosa.feature.mfcc(y=y_trimmed, sr=sr, n_mfcc=20, n_fft=2048, hop_length=512)\n",
    "            mfccs_normalized = (mfccs - np.mean(mfccs, axis=1, keepdims=True)) / np.std(mfccs, axis=1, keepdims=True)\n",
    "\n",
    "            mel_spec = librosa.feature.melspectrogram(y=y_trimmed, sr=sr, n_mels=40, n_fft=2048, hop_length=512, fmax=8000)\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            mel_spec_normalized = (mel_spec_db - np.mean(mel_spec_db)) / np.std(mel_spec_db)\n",
    "\n",
    "            features = np.vstack([mfccs_normalized, mel_spec_normalized])\n",
    "\n",
    "            if features.shape[1] > max_pad_len:\n",
    "                start = (features.shape[1] - max_pad_len) // 2\n",
    "                features = features[:, start:start + max_pad_len]\n",
    "            else:\n",
    "                pad_width = max_pad_len - features.shape[1]\n",
    "                features = np.pad(features, pad_width=((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "\n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Loading the audio file\n",
    "    y, sr = load_audio(audio_file_path)\n",
    "    if y is None:\n",
    "        return \"Error\", 0.0, {}\n",
    "\n",
    "    # Extract features from the audio\n",
    "    features = extract_features(y, sr)\n",
    "    if features is None:\n",
    "        return \"Error extracting features\", 0.0, {}\n",
    "\n",
    "    # Reshape features to match model input shape (add batch and channel dimensions)\n",
    "    features = features.reshape(1, features.shape[0], features.shape[1], 1)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(features)[0]\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    confidence = prediction[predicted_class]\n",
    "\n",
    "    # Create dictionary with all emotion probabilities\n",
    "    all_probs = {idx_to_emotion[i]: float(prob) for i, prob in enumerate(prediction)}\n",
    "\n",
    "    return idx_to_emotion[predicted_class], float(confidence), all_probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_emotion_with_visualization(audio_file_path, model_path='/content/best_emotion_model.h5', show_plots=True):\n",
    "\n",
    "    # Defining the emotion labels\n",
    "    emotion_labels = {\n",
    "        'sad': 0,\n",
    "        'surprised': 1,\n",
    "        'joyfully': 2,\n",
    "        'euphoric': 3\n",
    "    }\n",
    "    # Reverse the dictionary for prediction output\n",
    "    idx_to_emotion = {v: k for k, v in emotion_labels.items()}\n",
    "\n",
    "    # Load the model\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # Load and preprocess audio\n",
    "    y, sr = librosa.load(audio_file_path, sr=22050)\n",
    "    y = librosa.util.normalize(y)\n",
    "    y_trimmed, _ = librosa.effects.trim(y, top_db=20)\n",
    "\n",
    "    # Extract features\n",
    "    mfccs = librosa.feature.mfcc(y=y_trimmed, sr=sr, n_mfcc=20)\n",
    "    mfccs_normalized = (mfccs - np.mean(mfccs, axis=1, keepdims=True)) / np.std(mfccs, axis=1, keepdims=True)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y_trimmed, sr=sr, n_mels=40,fmax=8000)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_normalized = (mel_spec_db - np.mean(mel_spec_db)) / np.std(mel_spec_db)\n",
    "\n",
    "    features = np.vstack([mfccs_normalized, mel_spec_normalized])\n",
    "\n",
    "    # Apply same padding/cropping as in training\n",
    "    max_pad_len = 150\n",
    "    if features.shape[1] > max_pad_len:\n",
    "        start = (features.shape[1] - max_pad_len) // 2\n",
    "        features = features[:, start:start + max_pad_len]\n",
    "    else:\n",
    "        pad_width = max_pad_len - features.shape[1]\n",
    "        features = np.pad(features, pad_width=((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "\n",
    "    # Reshape features for the model\n",
    "    features_for_model = features.reshape(1, features.shape[0], features.shape[1], 1)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(features_for_model)[0]\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    confidence = prediction[predicted_class]\n",
    "    predicted_emotion = idx_to_emotion[predicted_class]\n",
    "\n",
    "    # Create dictionary with all emotion probabilities\n",
    "    all_probs = {idx_to_emotion[i]: float(prob) for i, prob in enumerate(prediction)}\n",
    "\n",
    "    # Visualize\n",
    "    if show_plots:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # Plot waveform\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.title(f\"Waveform - Predicted: {predicted_emotion} ({confidence:.2f})\")\n",
    "\n",
    "        time = np.arange(0, len(y_trimmed)) / sr\n",
    "        plt.plot(time, y_trimmed, color='b')\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "\n",
    "        # Plot MFCC\n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.title(\"MFCC Features\")\n",
    "        librosa.display.specshow(mfccs, sr=sr, x_axis='time')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.ylabel(\"MFCC Coeffs\")\n",
    "\n",
    "        # Plot Mel Spectrogram\n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.title(\"Mel Spectrogram\")\n",
    "        librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.ylabel(\"Frequency (Hz)\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot emotion probabilities\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(all_probs.keys(), all_probs.values())\n",
    "        plt.title(\"Emotion Prediction Probabilities\")\n",
    "        plt.ylabel(\"Probability\")\n",
    "        plt.ylim(0, 1)\n",
    "        for i, (emotion, prob) in enumerate(all_probs.items()):\n",
    "            plt.text(i, prob + 0.02, f\"{prob:.2f}\", ha='center')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return predicted_emotion, float(confidence), all_probs\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    audio_path = \"/content/sad.wav\"\n",
    "    emotion, confidence, probabilities = predict_emotion(audio_path)\n",
    "    print(f\"Predicted emotion: {emotion}\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "    print(\"All probabilities:\")\n",
    "    for emotion, prob in probabilities.items():\n",
    "        print(f\"{emotion} : {prob:.2f}\")\n",
    "\n",
    "    try:\n",
    "        emotion, confidence, probabilities = predict_emotion_with_visualization(audio_path)\n",
    "    except ImportError:\n",
    "        print(\"Some Import Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d54963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c254f299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcb728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36fa393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9835717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aac1db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
